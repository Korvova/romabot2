// webapp/src/App.jsx
import { useRef, useState } from 'react';
import './index.css';

function App() {
  const [log, setLog] = useState([]);
  const [started, setStarted] = useState(false);
  const pcRef    = useRef(null);
  const dcRef    = useRef(null);
  const wsRef    = useRef(null);
  const audioRef = useRef(null);
  const [isSpeaking, setIsSpeaking] = useState(false);

  const addLog = txt => setLog(l => [...l, txt]);

  const start = async () => {
    if (started) return;
    setStarted(true);
    addLog('â³ Loading configâ€¦');

    // 1) Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ assistant.txt Ð¸ tools.json
    const [{ instructions }, { tools: userTools }] = await Promise.all([
      fetch('/2/get-assistant').then(r => r.json()),
      fetch('/2/get-tools').then(r => r.json())
    ]);
    const assistantTxt = (instructions || '').trim();

console.log('ðŸ”‘ Loaded assistantTxt:', assistantTxt);
addLog(`ðŸ”‘ assistantTxt: ${assistantTxt}`);


    const hasQR = userTools.some(f => f.name === 'show_qr_code');
    const tools = hasQR
      ? userTools
      : [{
          type:'function',
          name:'show_qr_code',
          description:'ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ QRâ€‘ÐºÐ¾Ð´',
          parameters:{ type:'object', properties:{ qrUrl:{type:'string'}}, required:['qrUrl'] }
        }].concat(userTools);

    addLog('âœ… Config loaded');

    // 2) Ð”Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ URL Ð´Ð»Ñ WS
    const loc = window.location;
    const wsProtocol = loc.protocol === 'https:' ? 'wss:' : 'ws:';
  const wsUrl = `${wsProtocol}//${loc.host}${loc.pathname}ws`;
    const ws = new WebSocket(wsUrl);
    wsRef.current = ws;

    // Ð–Ð´Ñ‘Ð¼ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ñ WS, Ð¿Ð¾Ñ‚Ð¾Ð¼ ÑˆÐ»Ñ‘Ð¼ offer
    ws.onopen = () => {
      addLog('ðŸ”Œ WS open');
    };
    ws.onerror = () => addLog('âš ï¸ WS error');
    ws.onmessage = async ({ data }) => {
      const msg = JSON.parse(data);
      if (msg.type === 'answer') {
        addLog('â† got SDP answer');
        await pcRef.current.setRemoteDescription({ type:'answer', sdp: msg.sdp });
      }
    };

    // 3) RTCPeerConnection Ð¸ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½
    const pc = new RTCPeerConnection({ iceServers:[{ urls:'stun:stun.l.google.com:19302' }] });
    pcRef.current = pc;
    const mic = await navigator.mediaDevices.getUserMedia({ audio:true });
    pc.addTrack(mic.getTracks()[0]);
    addLog('ðŸŽ¤ Mic added');

    pc.ontrack = e => {
      audioRef.current.srcObject = e.streams[0];
      addLog('ðŸŽ§ Remote audio');
      setIsSpeaking(true);
      setTimeout(()=>setIsSpeaking(false),1500);
    };

    // 4) DataChannel Ð´Ð»Ñ OpenAI
    const dc = pc.createDataChannel('oai-events');
    dcRef.current = dc;
    dc.onopen = () => {
      addLog('ðŸ“¡ DC open');
      const oaTools = tools.map(({ type,name,description,parameters })=>({ type,name,description,parameters }));
      dc.send(JSON.stringify({
        type: 'session.update',
        session: {
          instructions: assistantTxt,
          voice: 'alloy',
          turn_detection: { type:'server_vad' },
          input_audio_transcription: { model:'whisper-1' },
          output_audio_format: 'pcm16',
          modalities: ['text','audio'],
          tools: oaTools,
          tool_choice: 'auto'
        }
      }));
    };
    dc.onmessage = ({ data }) => {
      const ev = JSON.parse(data);




    if (ev.type === 'response.content_part.added') {
       setIsSpeaking(true);
     }
     // Ð§Ð°ÑÑ‚Ð¸Ñ‡Ð½Ð°Ñ Ñ€Ð°ÑÐ¿ÐµÑ‡Ð°Ñ‚ÐºÐ° Ð°ÑƒÐ´Ð¸Ð¾â€‘Ñ‚ÐµÐºÑÑ‚Ð° (ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿Ð¾Ð´ÑÐ²ÐµÑ‚Ð¸Ñ‚ÑŒ Ñ€ÐµÑ‡ÑŒ)
    if (ev.type === 'response.audio_transcript.delta') {
       setIsSpeaking(true);
       addLog(`Roma: ${ev.delta}`);
     }
    // ÐšÐ¾Ð³Ð´Ð° OpenAI Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ð» Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ
     if (ev.type === 'output_audio_buffer.stopped') {
       setIsSpeaking(false);
     }



    };

    // 5) SDP handshake
    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);
    addLog('â†’ sending SDP offer');
    // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ„Ñ„ÐµÑ€ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾ÑÐ»Ðµ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ñ WS
    ws.onopen = () => {
      addLog('ðŸ”Œ WS open');
      ws.send(JSON.stringify({ type:'offer', sdp: offer.sdp }));
    };
  };

  const stop = () => {
    dcRef.current?.close();
    pcRef.current?.close();
    wsRef.current?.close();
    setStarted(false);
    setIsSpeaking(false);
    addLog('ðŸ›‘ Stopped');
  };

  return (
    <div className="app-container">


<img
  id="silent-avatar"
  src="/2/roma-silent.gif"
  className="avatar silent"
  style={{ zIndex: isSpeaking ? 0 : 1 }}
/>
<img
  id="speak-avatar"
  src="/2/roma-speak.gif"
  className="avatar speaking"
  style={{ zIndex: isSpeaking ? 1 : 0 }}
/>




      <div id="controls">
        <button onClick={start} disabled={started}>Start Talking</button>
        <button onClick={stop}   disabled={!started}>Stop</button>
        <div id="log">{log.map((l,i)=><p key={i}>{l}</p>)}</div>
      </div>

      <audio ref={audioRef} autoPlay style={{ display:'none' }} />
    </div>
  );
}

export default App;
